#!/usr/bin/env python3
##############################################################################
# Project:  arrayfunc
# Module:   benchfuncs.py
# Purpose:  Benchmark functions for miscellaneous functions.
# Language: Python 3.4
# Date:     16-Sep-2014.
# Ver:      14-Jun-2018.
#
###############################################################################
#
#   Copyright 2014 - 2018    Michael Griffin    <m12.griffin@gmail.com>
#
#   Licensed under the Apache License, Version 2.0 (the "License");
#   you may not use this file except in compliance with the License.
#   You may obtain a copy of the License at
#
#       http://www.apache.org/licenses/LICENSE-2.0
#
#   Unless required by applicable law or agreed to in writing, software
#   distributed under the License is distributed on an "AS IS" BASIS,
#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#   See the License for the specific language governing permissions and
#   limitations under the License.
#
##############################################################################

import time
import array
import itertools
import math
import platform

import arrayfunc

##############################################################################

# The size of test array to use.
ARRAYSIZE = 1000000

##############################################################################

# This defines which functions and which array types have SIMD versions.
# This was generated by 'docsgen.py' and must be transferred here manually if
# the C code is changed. 
# Asum and asumov were modified manually to account for the test method used.
# Asum must be all false, and asumov added to stand in its place. Asumov is
# asum with overflow checking disabled.
SIMDFuncs = {
	'amax': {'q': False, 'd': True, 'l': False, 'i': True, 'I': True, 'L': False, 'H': True, 'b': True, 'h': True, 'f': True, 'Q': False, 'B': True}, 
	'aany': {'q': False, 'd': True, 'l': False, 'i': True, 'I': False, 'L': False, 'H': False, 'b': True, 'h': True, 'f': True, 'Q': False, 'B': False}, 
	'asum': {'q': False, 'd': False, 'l': False, 'i': False, 'I': False, 'L': False, 'H': False, 'b': False, 'h': False, 'f': False, 'Q': False, 'B': False}, 
	'asumov': {'q': False, 'd': True, 'l': False, 'i': False, 'I': False, 'L': False, 'H': False, 'b': False, 'h': False, 'f': True, 'Q': False, 'B': False}, 
	'amin': {'q': False, 'd': True, 'l': False, 'i': True, 'I': True, 'L': False, 'H': True, 'b': True, 'h': True, 'f': True, 'Q': False, 'B': True}, 
	'aall': {'q': False, 'd': True, 'l': False, 'i': True, 'I': False, 'L': False, 'H': False, 'b': True, 'h': True, 'f': True, 'Q': False, 'B': False}, 
	'findindex': {'q': False, 'd': True, 'l': False, 'i': True, 'I': False, 'L': False, 'H': False, 'b': True, 'h': True, 'f': True, 'Q': False, 'B': False}
	}

##############################################################################

# These are set to target a balanced execution time for the different tests.
# The target was to cause each test to run for approximately 0.1 second on a
# typical PC. Tests that run too quickly risk poor accuracy due to platform
# timer resolution. 
calibrationdata = {
'aall' : (12, 121, 1449),
'aany' : (24, 237, 3030),
'afilter' : (1, 114),
'amax' : (5, 165, 2439),
'amin' : (5, 162, 1587),
'asum' : (11, 168, 175),
'compress' : (3, 69),
'count' : (1, 161),
'cycle' : (2, 104),
'dropwhile' : (1, 115),
'findindex' : (6, 110, 1389),
'findindices' : (3, 114),
'repeat' : (5, 178),
'takewhile' : (1, 240)
}

arraycodes = ['b', 'B', 'h', 'H', 'i', 'I', 'l', 'L', 'q', 'Q', 'f', 'd']

##############################################################################

def comptype(arraycode, cval):
	"""Return the compare value in the correct type.
	"""
	if arraycode in ('f', 'd'):
		return float(cval)
	else:
		return int(cval)


##############################################################################

########################################################
def benchcount(arraycode):
	"""Benchmark the count function.
	"""
	# This is used to prevent integers exceeding the maximum size
	# for smaller word sizes.
	rollmasks = {
		'b' : 0x7f,
		'B' : 0xff, 
		'h' : 0x7fff, 
		'H' : 0xffff, 
		'i' : 0x7fffffff, 
		'I' : 0xffffffff, 
		'l' : 0x7fffffffffffffff, 
		'L' : 0xffffffffffffffff, 
		'q' : 0x7fffffffffffffff, 
		'Q' : 0xffffffffffffffff, 
		'f' : 0xffffffffffffffff, 
		'd' : 0xffffffffffffffff, 
	}

	# These are set to target a balanced execution time for the different tests.
	pyitercounts = calibrationdata['count'][0]
	afunccounts = calibrationdata['count'][1]

	data = array.array(arraycode, itertools.repeat(0, ARRAYSIZE))

	# Native Python time.
	starttime = time.perf_counter()
	# We need to prevent overflows.
	if arraycode in ('b', 'B', 'h', 'H'):
		mask = rollmasks[arraycode]
		for i in range(pyitercounts):
			for x, y in zip(itertools.count(0), itertools.repeat(0, ARRAYSIZE)):
				data[x] = x & mask
	else:
		for i in range(pyitercounts):
			for x, y in zip(itertools.count(0), itertools.repeat(0, ARRAYSIZE)):
				data[x] = x
	endtime = time.perf_counter()

	pythontime = (endtime - starttime) / pyitercounts

	data = array.array(arraycode, itertools.repeat(0, ARRAYSIZE))
	# Arrayfunc time.
	starttime = time.perf_counter()
	for i in range(afunccounts):
		arrayfunc.count(data, 0) 
	endtime = time.perf_counter()

	functime = (endtime - starttime) / afunccounts


	return (pythontime, functime, None)


#############################################################################

def benchcycle(arraycode):
	"""Benchmark the cycle function.
	"""
	# These are set to target a balanced execution time for the different tests.
	pyitercounts = calibrationdata['cycle'][0]
	afunccounts = calibrationdata['cycle'][1]

	data = array.array(arraycode, itertools.repeat(0, ARRAYSIZE))

	startcycle = comptype(arraycode, 0)
	endcycle = comptype(arraycode, 127)

	# Native Python time.
	cycledata = list(range(128))
	starttime = time.perf_counter()
	for i in range(pyitercounts):
		for x, y in zip(itertools.cycle(cycledata), itertools.repeat(0, ARRAYSIZE)):
			data[x] = x
	endtime = time.perf_counter()

	pythontime = (endtime - starttime) / pyitercounts

	data = array.array(arraycode, itertools.repeat(0, ARRAYSIZE))
	# Arrayfunc time.
	starttime = time.perf_counter()
	for i in range(afunccounts):
		arrayfunc.cycle(data, startcycle, endcycle)
	endtime = time.perf_counter()

	functime = (endtime - starttime) / afunccounts


	return (pythontime, functime, None)


#############################################################################

def benchrepeat(arraycode):
	"""Benchmark the repeat function.
	"""
	# These are set to target a balanced execution time for the different tests.
	pyitercounts = calibrationdata['repeat'][0]
	afunccounts = calibrationdata['repeat'][1]

	data = array.array(arraycode, itertools.repeat(0, ARRAYSIZE))

	compval = comptype(arraycode, 10)

	# Native Python time.
	starttime = time.perf_counter()
	for i in range(pyitercounts):
		data = array.array(arraycode, itertools.repeat(0, ARRAYSIZE))
	endtime = time.perf_counter()

	pythontime = (endtime - starttime) / pyitercounts

	data = array.array(arraycode, itertools.repeat(0, ARRAYSIZE))
	# Arrayfunc time.
	starttime = time.perf_counter()
	for i in range(afunccounts):
		arrayfunc.repeat(data, compval)
	endtime = time.perf_counter()

	functime = (endtime - starttime) / afunccounts


	return (pythontime, functime, None)




#############################################################################

def benchafilter(arraycode):
	"""Benchmark the afilter function.
	"""
	# These are set to target a balanced execution time for the different tests.
	pyitercounts = calibrationdata['afilter'][0]
	afunccounts = calibrationdata['afilter'][1]

	cycledata = list(range(128))
	data = array.array(arraycode, itertools.repeat(0, ARRAYSIZE))
	for x, y in zip(itertools.cycle(cycledata), itertools.repeat(0, ARRAYSIZE)):
		data[x] = x
	dataout = array.array(arraycode, itertools.repeat(0, ARRAYSIZE))

	compval = comptype(arraycode, 50)

	# Native Python time.
	starttime = time.perf_counter()
	for i in range(pyitercounts):
		dataout = array.array(arraycode, filter(lambda x: x < compval, data))
	endtime = time.perf_counter()

	pythontime = (endtime - starttime) / pyitercounts

	dataout = array.array(arraycode, itertools.repeat(0, ARRAYSIZE))
	# Arrayfunc time.
	starttime = time.perf_counter()
	for i in range(afunccounts):
		x = arrayfunc.afilter('<', data, dataout, compval)
	endtime = time.perf_counter()

	functime = (endtime - starttime) / afunccounts


	return (pythontime, functime, None)


#############################################################################

def benchcompress(arraycode):
	"""Benchmark the compress function.
	"""
	# These are set to target a balanced execution time for the different tests.
	pyitercounts = calibrationdata['compress'][0]
	afunccounts = calibrationdata['compress'][1]

	cycledata = list(range(128))
	compdata = array.array(arraycode, [1,0,1,0])
	data = array.array(arraycode, itertools.repeat(0, ARRAYSIZE))
	for x, y in zip(itertools.cycle(cycledata), itertools.repeat(0, ARRAYSIZE)):
		data[x] = x
	dataout = array.array(arraycode, itertools.repeat(0, ARRAYSIZE))
	pycomp = array.array(arraycode, (x for x,y in zip(itertools.cycle(compdata), itertools.repeat(0, ARRAYSIZE))))

	# Native Python time.
	starttime = time.perf_counter()
	for i in range(pyitercounts):
		dataout = array.array(arraycode, itertools.compress(data, pycomp))
	endtime = time.perf_counter()

	pythontime = (endtime - starttime) / pyitercounts

	dataout = array.array(arraycode, itertools.repeat(0, ARRAYSIZE))
	# Arrayfunc time.
	starttime = time.perf_counter()
	for i in range(afunccounts):
		x = arrayfunc.compress(data, dataout, compdata)
	endtime = time.perf_counter()

	functime = (endtime - starttime) / afunccounts


	return (pythontime, functime, None)


#############################################################################

def benchdropwhile(arraycode):
	"""Benchmark the dropwhile function.
	"""
	# These are set to target a balanced execution time for the different tests.
	pyitercounts = calibrationdata['dropwhile'][0]
	afunccounts = calibrationdata['dropwhile'][1]

	data = array.array(arraycode, itertools.chain(itertools.repeat(5, ARRAYSIZE // 2), itertools.repeat(50, ARRAYSIZE // 2)))
	dataout = array.array(arraycode, itertools.repeat(0, ARRAYSIZE))
	compval = comptype(arraycode, 10)

	# Native Python time.
	starttime = time.perf_counter()
	for i in range(pyitercounts):
		dataout = array.array(arraycode, itertools.dropwhile(lambda x : x < compval, data))
	endtime = time.perf_counter()

	pythontime = (endtime - starttime) / pyitercounts

	dataout = array.array(arraycode, itertools.repeat(0, ARRAYSIZE))
	# Arrayfunc time.
	starttime = time.perf_counter()
	for i in range(afunccounts):
		x = arrayfunc.dropwhile('<', data, dataout, compval)
	endtime = time.perf_counter()

	functime = (endtime - starttime) / afunccounts


	return (pythontime, functime, None)



#############################################################################

def benchtakewhile(arraycode):
	"""Benchmark the takewhile function.
	"""
	# These are set to target a balanced execution time for the different tests.
	pyitercounts = calibrationdata['takewhile'][0]
	afunccounts = calibrationdata['takewhile'][1]

	data = array.array(arraycode, itertools.chain(itertools.repeat(5, ARRAYSIZE // 2), itertools.repeat(50, ARRAYSIZE // 2)))
	dataout = array.array(arraycode, itertools.repeat(0, ARRAYSIZE))
	compval = comptype(arraycode, 10)

	# Native Python time.
	starttime = time.perf_counter()
	for i in range(pyitercounts):
		dataout = array.array(arraycode, itertools.takewhile(lambda x : x < compval, data))
	endtime = time.perf_counter()

	pythontime = (endtime - starttime) / pyitercounts

	dataout = array.array(arraycode, itertools.repeat(0, ARRAYSIZE))
	# Arrayfunc time.
	starttime = time.perf_counter()
	for i in range(afunccounts):
		x = arrayfunc.takewhile('<', data, dataout, compval)
	endtime = time.perf_counter()

	functime = (endtime - starttime) / afunccounts


	return (pythontime, functime, None)




#############################################################################

def benchaany(arraycode):
	"""Benchmark the aany function.
	"""
	# These are set to target a balanced execution time for the different tests.
	pyitercounts = calibrationdata['aany'][0]
	afunccounts = calibrationdata['aany'][1]
	simdcounts = calibrationdata['aany'][2]

	data = array.array(arraycode, itertools.chain(itertools.repeat(0, ARRAYSIZE // 2), itertools.repeat(10, ARRAYSIZE // 2)))
	compval = comptype(arraycode, 5)

	# Native Python time.
	starttime = time.perf_counter()
	for i in range(pyitercounts):
		x = any(data)
	endtime = time.perf_counter()

	pythontime = (endtime - starttime) / pyitercounts

	# Arrayfunc time.
	starttime = time.perf_counter()
	for i in range(afunccounts):
		x = arrayfunc.aany('>', data, compval, nosimd=True)
	endtime = time.perf_counter()

	functime = (endtime - starttime) / afunccounts

	# Arrayfunc time, with SIMD.
	if arrayfunc.simdsupport.hassimd:
		starttime = time.perf_counter()
		for i in range(simdcounts):
			x = arrayfunc.aany('>', data, compval)
		endtime = time.perf_counter()

		simdtime = (endtime - starttime) / simdcounts
	else:
		simdtime = None


	return (pythontime, functime, simdtime)


#############################################################################

def benchaall(arraycode):
	"""Benchmark the aall function.
	"""
	# These are set to target a balanced execution time for the different tests.
	pyitercounts = calibrationdata['aall'][0]
	afunccounts = calibrationdata['aall'][1]
	simdcounts = calibrationdata['aall'][2]

	data = array.array(arraycode, itertools.repeat(10, ARRAYSIZE))
	compval = comptype(arraycode, 5)

	# Native Python time.
	starttime = time.perf_counter()
	for i in range(pyitercounts):
		x = all(data)
	endtime = time.perf_counter()

	pythontime = (endtime - starttime) / pyitercounts

	# Arrayfunc time.
	starttime = time.perf_counter()
	for i in range(afunccounts):
		x = arrayfunc.aall('>', data, compval, nosimd=True)
	endtime = time.perf_counter()

	functime = (endtime - starttime) / afunccounts

	# Arrayfunc time, with SIMD.
	if arrayfunc.simdsupport.hassimd:
		starttime = time.perf_counter()
		for i in range(simdcounts):
			x = arrayfunc.aall('>', data, compval)
		endtime = time.perf_counter()

		simdtime = (endtime - starttime) / simdcounts
	else:
		simdtime = None

	return (pythontime, functime, simdtime)


#############################################################################

def benchamax(arraycode):
	"""Benchmark the amax function.
	"""
	# These are set to target a balanced execution time for the different tests.
	pyitercounts = calibrationdata['amax'][0]
	afunccounts = calibrationdata['amax'][1]
	simdcounts = calibrationdata['amax'][2]

	cycledata = list(range(128))
	data = array.array(arraycode, itertools.repeat(0, ARRAYSIZE))
	for x, y in zip(itertools.cycle(cycledata), itertools.repeat(0, ARRAYSIZE)):
		data[x] = x

	# Native Python time.
	starttime = time.perf_counter()
	for i in range(pyitercounts):
		x = max(data)
	endtime = time.perf_counter()

	pythontime = (endtime - starttime) / pyitercounts

	# Arrayfunc time.
	starttime = time.perf_counter()
	for i in range(afunccounts):
		x = arrayfunc.amax(data, nosimd=True)
	endtime = time.perf_counter()

	functime = (endtime - starttime) / afunccounts

	# Arrayfunc time, with SIMD.
	if arrayfunc.simdsupport.hassimd:
		starttime = time.perf_counter()
		for i in range(simdcounts):
			x = arrayfunc.amax(data)
		endtime = time.perf_counter()

		simdtime = (endtime - starttime) / simdcounts
	else:
		simdtime = None


	return (pythontime, functime, simdtime)


#############################################################################

def benchamin(arraycode):
	"""Benchmark the amin function.
	"""
	# These are set to target a balanced execution time for the different tests.
	pyitercounts = calibrationdata['amin'][0]
	afunccounts = calibrationdata['amin'][1]
	simdcounts = calibrationdata['amin'][2]

	cycledata = list(range(128))
	data = array.array(arraycode, itertools.repeat(0, ARRAYSIZE))
	for x, y in zip(itertools.cycle(cycledata), itertools.repeat(0, ARRAYSIZE)):
		data[x] = x

	# Native Python time.
	starttime = time.perf_counter()
	for i in range(pyitercounts):
		x = min(data)
	endtime = time.perf_counter()

	pythontime = (endtime - starttime) / pyitercounts

	# Arrayfunc time.
	starttime = time.perf_counter()
	for i in range(afunccounts):
		x = arrayfunc.amin(data, nosimd=True)
	endtime = time.perf_counter()

	functime = (endtime - starttime) / afunccounts

	# Arrayfunc time, with SIMD.
	if arrayfunc.simdsupport.hassimd:
		starttime = time.perf_counter()
		for i in range(simdcounts):
			x = arrayfunc.amin(data)
		endtime = time.perf_counter()

		simdtime = (endtime - starttime) / simdcounts
	else:
		simdtime = None


	return (pythontime, functime, simdtime)


#############################################################################

def benchfindindex(arraycode):
	"""Benchmark the findindex function.
	"""
	# These are set to target a balanced execution time for the different tests.
	pyitercounts = calibrationdata['findindex'][0]
	afunccounts = calibrationdata['findindex'][1]
	simdcounts = calibrationdata['findindex'][2]

	compval = comptype(arraycode, 10)
	data = array.array(arraycode, itertools.repeat(0, ARRAYSIZE))
	data[-1] = compval

	# Native Python time.
	starttime = time.perf_counter()
	for i in range(pyitercounts):
		x = data.index(compval)
	endtime = time.perf_counter()

	pythontime = (endtime - starttime) / pyitercounts

	# Arrayfunc time.
	starttime = time.perf_counter()
	for i in range(afunccounts):
		x = arrayfunc.findindex('==', data, compval, nosimd=True)
	endtime = time.perf_counter()

	functime = (endtime - starttime) / afunccounts

	# Arrayfunc time, with SIMD.
	if arrayfunc.simdsupport.hassimd:
		starttime = time.perf_counter()
		for i in range(simdcounts):
			x = arrayfunc.findindex('==', data, compval)
		endtime = time.perf_counter()

		simdtime = (endtime - starttime) / simdcounts
	else:
		simdtime = None


	return (pythontime, functime, simdtime)


#############################################################################

def benchfindindices(arraycode):
	"""Benchmark the findindices function.
	"""
	# These are set to target a balanced execution time for the different tests.
	pyitercounts = calibrationdata['findindices'][0]
	afunccounts = calibrationdata['findindices'][1]

	compval = comptype(arraycode, 10)
	data = array.array(arraycode, itertools.repeat(0, ARRAYSIZE))
	data[-1] = compval
	dataout = array.array('q', itertools.repeat(0, ARRAYSIZE))

	# Native Python time.
	starttime = time.perf_counter()
	for i in range(pyitercounts):
		z = 0
		for x in data:
			if x == compval:
				dataout[z] = z
				z += 1
	endtime = time.perf_counter()

	pythontime = (endtime - starttime) / pyitercounts

	dataout = array.array('q', itertools.repeat(0, ARRAYSIZE))

	# Arrayfunc time.
	starttime = time.perf_counter()
	for i in range(afunccounts):
		x = arrayfunc.findindices('==', data, dataout, compval)
	endtime = time.perf_counter()

	functime = (endtime - starttime) / afunccounts


	return (pythontime, functime, None)


#############################################################################

def benchasum(arraycode):
	"""Benchmark the asum function.
	"""
	# These are set to target a balanced execution time for the different tests.
	pyitercounts = calibrationdata['asum'][0]
	afunccounts = calibrationdata['asum'][1]
	simdcounts = calibrationdata['asum'][2]

	compval = comptype(arraycode, 10)
	data = array.array(arraycode, itertools.repeat(0, ARRAYSIZE))
	data[-1] = compval

	# Native Python time.
	starttime = time.perf_counter()
	for i in range(pyitercounts):
		x = sum(data)
	endtime = time.perf_counter()

	pythontime = (endtime - starttime) / pyitercounts

	# Arrayfunc time.
	starttime = time.perf_counter()
	for i in range(afunccounts):
		x = arrayfunc.asum(data, nosimd=True)
	endtime = time.perf_counter()

	functime = (endtime - starttime) / afunccounts

	# Arrayfunc time, with SIMD. This won't actually be any faster
	# since we must disable overflow checking to enable SIMD.
	if arrayfunc.simdsupport.hassimd:
		starttime = time.perf_counter()
		for i in range(simdcounts):
			x = arrayfunc.asum(data)
		endtime = time.perf_counter()

		simdtime = (endtime - starttime) / simdcounts
	else:
		simdtime = None


	return (pythontime, functime, simdtime)


#############################################################################

def benchasumov(arraycode):
	"""Benchmark the asum function with overflow checking disabled.
	"""
	# These are set to target a balanced execution time for the different tests.
	pyitercounts = calibrationdata['asum'][0]
	afunccounts = calibrationdata['asum'][1]
	simdcounts = calibrationdata['asum'][2]

	compval = comptype(arraycode, 10)
	data = array.array(arraycode, itertools.repeat(0, ARRAYSIZE))
	data[-1] = compval

	# Native Python time.
	starttime = time.perf_counter()
	for i in range(pyitercounts):
		x = sum(data)
	endtime = time.perf_counter()

	pythontime = (endtime - starttime) / pyitercounts

	# Arrayfunc time.
	starttime = time.perf_counter()
	for i in range(afunccounts):
		x = arrayfunc.asum(data, matherrors=True, nosimd=True)
	endtime = time.perf_counter()

	functime = (endtime - starttime) / afunccounts

	# Arrayfunc time, with SIMD.
	if arrayfunc.simdsupport.hassimd:
		starttime = time.perf_counter()
		for i in range(simdcounts):
			x = arrayfunc.asum(data, matherrors=True)
		endtime = time.perf_counter()

		simdtime = (endtime - starttime) / simdcounts
	else:
		simdtime = None


	return (pythontime, functime, simdtime)

#############################################################################


def dataformat(val):
	"""Format the output data.
	"""
	if val >= 10.0:
		return '%0.0f' % val
	else:
		return '%0.1f' % val


#############################################################################

TestResults = {}
FuncNames = []

TestNames = [('count', benchcount), ('cycle', benchcycle), ('repeat', benchrepeat),
			('afilter', benchafilter), ('compress', benchcompress), ('dropwhile', benchdropwhile),
			('takewhile', benchtakewhile), ('aany', benchaany), ('aall', benchaall),
			('amax', benchamax), ('amin', benchamin), 
			('findindex', benchfindindex), ('findindices', benchfindindices),
			('asum', benchasum),
			('asumov', benchasumov)]


TestNames.sort(key= lambda x: x[0])
TestLabels = [x for x,y in TestNames]


PyResults = {}
FuncResults = {}
FuncResultsSIMD = {}
RelativeResults = {}
RelativeResultsSIMD = {}
RelativeCompareSIMD = {}

numstats = []
numstatssimd = []
numstatssimdcomp = []

for testname, testfunc in TestNames:
	print(testname)
	PyResults[testname] = {}
	FuncResults[testname] = {}
	FuncResultsSIMD[testname] = {}
	RelativeResults[testname] = {}
	RelativeResultsSIMD[testname] = {}
	RelativeCompareSIMD[testname] = {}
	
	Results = {}
	for i in arraycodes:
		pyval, funcval, simdval = testfunc(i)

		PyResults[testname][i] = '%0.0f' % (pyval * 1000000.0)
		FuncResults[testname][i] = '%0.0f' % (funcval * 1000000.0)

		# Arrayfunc without SIMD.
		relval = pyval / funcval
		RelativeResults[testname][i] = dataformat(relval)
		numstats.append(relval)


		# Arrayfunc with SIMD.
		if (simdval != None) and SIMDFuncs.get(testname, False) and SIMDFuncs[testname].get(i, False):
			FuncResultsSIMD[testname][i] = '%0.0f' % (simdval * 1000000.0)

			relsimd = pyval / simdval
			numstatssimd.append(relsimd)
			RelativeResultsSIMD[testname][i] = dataformat(relsimd)

			# Compare Arrayfunc with SIMD to Arrayfunc without SIMD.
			relsimdcomp = funcval / simdval
			numstatssimdcomp.append(relsimdcomp)
			RelativeCompareSIMD[testname][i] = dataformat(relsimdcomp)
		else:
			FuncResultsSIMD[testname][i] = '   '
			RelativeResultsSIMD[testname][i] = '   '
			RelativeCompareSIMD[testname][i] = '   '


##############################################################################

# Write the results to disk.
def WriteResults(outputfile, columnwidth, testresults):
	"""Parameters: outputfile (file object) = The file object for the output file.
			columnwidth (integer) = The width of the data columns.
			testresults (dict of dicts) = The test results.
	"""
	tableheader = {'func' : 'function', 'b' : 'b', 'B' : 'B', 'h' : 'h', 'H' : 'H', 
		'i' : 'i', 'I' : 'I', 'l' : 'l', 'L' : 'L', 'q' : 'q', 'Q' : 'Q', 'f' : 'f', 'd' : 'd'}
	tablesep = dict.fromkeys(arraycodes, '=' * columnwidth)
	tablesep.update({'func' : '==========='})
	tableformat = '%(func)11s ' + ' '.join(['%(' + x + (')%is' % columnwidth) for x in arraycodes]) + '\n'

	outputfile.write(tableformat % tablesep)
	outputfile.write(tableformat % tableheader)
	outputfile.write(tableformat % tablesep)

	for func in TestLabels:
		bc = testresults[func]
		benchdata = {'func' : func}
		benchdata.update(bc)
		outputfile.write(tableformat % benchdata)


	outputfile.write(tableformat % tablesep)


##############################################################################

# Write out the platform data to keep track of what platform the
def WritePlatformSignature(f):
	# test was run on.
	# 'Linux'
	f.write('Operating System: ' + platform.system() + '\n')

	# 'Linux-4.4.0-79-generic-x86_64-with-Ubuntu-16.04-xenial'
	f.write('Platform: ' + platform.platform() + '\n')

	# ('64bit', 'ELF')
	f.write('Word size: ' + platform.architecture()[0] + '\n')

	# 'GCC 5.4.0 20160609'
	f.write('Compiler: ' + platform.python_compiler() + '\n')

	# '4.4.0-79-generic'
	f.write('Python release: ' + platform.release() + '\n')
	f.write('\n')


	f.write('Asumov in the following indicates asum with overflow checking turned off.\n\n\n')


##############################################################################

# Calculate the stats.
def CalcStats(benchstats):
	avgval = sum(benchstats) / len(benchstats)
	maxval = max(benchstats)
	minval = min(benchstats)

	return avgval, maxval, minval

# Write the 
def WriteSummary(outputfile, avgval, maxval, minval):
	# Summary
	outputfile.write('\n\n\n')
	outputfile.write('=========== ========\n')
	outputfile.write('Stat         Value\n')
	outputfile.write('=========== ========\n')
	outputfile.write('Average:    %0.0f\n' % avgval)
	outputfile.write('Maximum:    %0.0f\n' % maxval)
	outputfile.write('Minimum:    %0.1f\n' % minval)
	outputfile.write('Array size: %d\n' % ARRAYSIZE)
	outputfile.write('=========== ========\n')
	

##############################################################################

# These are the benchmark results which compares native Python speed to
# ArrayFunc speed.

with open('benchfuncs.txt', 'w') as f:

	WritePlatformSignature(f)

	f.write('Arrayfunc faster than Python factor.\n')
	WriteResults(f, 5, RelativeResults)

	# Stats for all array types.
	avgval, maxval, minval = CalcStats(numstats)

	# Summary
	WriteSummary(f, avgval, maxval, minval)

	if arrayfunc.simdsupport.hassimd:
		f.write('\n\n\n\n')

		f.write('Arrayfunc with SIMD faster than Python factor.\n')
		WriteResults(f, 5, RelativeResultsSIMD)

		# Stats for all array types with SIMD.
		avgvalsimd, maxvalsimd, minvalsimd = CalcStats(numstatssimd)

		# Summary with SIMD.
		WriteSummary(f, avgvalsimd, maxvalsimd, minvalsimd)



		f.write('\n\n\n\n')

		f.write('Arrayfunc with SIMD faster than Arrayfunc without SIMD factor.\n')
		f.write('SIMD is not supported for all array types, so some types will not show a speed up.\n')
		WriteResults(f, 5, RelativeCompareSIMD)

		# Stats for all array types with and without SIMD.
		avgvalcomp, maxvalcomp, minvalcomp = CalcStats(numstatssimdcomp)

		# Summary with and without SIMD.
		WriteSummary(f, avgvalcomp, maxvalcomp, minvalcomp)



	# This provides the absolute time for executing each benchmark in order to
	# provide relative benchmarks for comparing different platforms. 


	f.write('\n\n\n\n')

	f.write('Python native time in micro-seconds.\n')
	WriteResults(f, 8, PyResults)

	f.write('\n\nArrayfunc without SIMD time in micro-seconds.\n')
	WriteResults(f, 8, FuncResults)

	if arrayfunc.simdsupport.hassimd:
		f.write('\n\nArrayfunc with SIMD time in micro-seconds.\n')
		WriteResults(f, 8, FuncResultsSIMD)

##############################################################################
